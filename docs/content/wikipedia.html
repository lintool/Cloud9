<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title>Cloud9: A MapReduce Library for Hadoop &#187; Working with Wikipedia</title>
<style type="text/css" media="screen">@import url( ../style.css );</style>
</head>

<body>

<div id="wrap">
<div id="container" class="one-column" >

<!-- header START -->

<div id="header">
<div id="caption">
<h1 id="title" style="color: white;">Cloud<sup><small>9</small></sup></h1>
<div id="tagline">A MapReduce Library for Hadoop</div>
</div>

<div class="fixed"></div>

</div>

<!-- header END -->

<!-- navigation START -->

<div id="navigation">

<script type="text/javascript" src="menu.js">
</script>

<div class="fixed"></div>

</div>

<!-- navigation END -->



<!-- content START -->

<div id="content">



	<!-- main START -->

	<div id="main">


<!--- START MAIN CONTENT HERE -->

<h2>Working with Wikipedia</h2>

<div class="post">
<div class="content">

<p>For several reasons, Wikipedia is among the most popular datasets
to process with Hadoop: it's big, it contains a lot of text, and it
has a rich link structure.  And perhaps most important of all,
Wikipedia can be freely downloaded by anyone.  Where do you get it?
See <a href="http://en.wikipedia.org/wiki/Wikipedia_database">this
page</a> for information about raw Wikipedia dumps in XML.  Direct
access to English Wikipedia dumps can be
found <a href="http://download.wikimedia.org/enwiki/">here</a>.</p>

<p>In this guide, we'll be working with the XML dump of English
Wikipedia dated April 5, 2011 (<code>enwiki-20110405-pages-articles.xml</code>).
Note that Wikipedia is distributed as a single, large XML file
compressed with bzip2.  We'll want to repack the dataset as
block-compressed SequenceFiles (see below), so you should uncompress
the file before putting it into HDFS.</p>

<p>Aside: why do we want to uncompress the XML file?  Although
bzip2-compressed files are splittable, the decompression algorithm is
slow and cannot keep up with the streaming disk reads that are common
in Hadoop jobs.  See
a <a href="http://www.cloudera.com/blog/2009/11/hadoop-at-twitter-part-1-splittable-lzo-compression/">Cloudera
blog post about this subject</a> for more details.</p>

<p>As a sanity check, the first thing you might want to do is count
how many pages there are in this particular version of Wikipedia:</p>

<pre>
hadoop jar cloud9.jar edu.umd.cloud9.collection.wikipedia.DemoCountWikipediaPages \
  -libjars bliki-core-3.0.16.jar,commons-cli-1.2.jar,commons-lang-2.6.jar \
  -input Wikipedia/raw/enwiki-20110405-pages-articles.xml
</pre>

<p>Conveniently enough, Cloud<sup><small>9</small></sup> provides a
<code>WikipediaPageInputFormat</code> that automatically parses the XML dump file,
so that your mapper will be presented <code>WikipediaPage</code> objects to
process.  Note that you have to include dependencies with
the <code>-libjars</code> option.</p>

<p>After running the above program, you'll see the follow statistics
stored in counters:</p>

<table>
<tr><td class="myheader"><b>Type</b></td>
    <td class="myheader"><b>Count</b></td>
</tr>

<tr>
<td class="mycell" align="left">Redirect pages</td>
<td class="mycell" style="text-align:right;">4,871,488</td>
</tr>

<tr>
<td class="mycell" align="left">Disambiguation pages</td>
<td class="mycell" style="text-align:right;">124,961</td>
</tr>

<tr>
<td class="mycell" align="left">Empty pages</td>
<td class="mycell" style="text-align:right;">465</td>
</tr>

<tr>
<td class="mycell" align="left">Stub articles</td>
<td class="mycell" style="text-align:right;">1,544,446</td>
</tr>

<tr>
<td class="mycell" align="left">Total articles (including stubs)</td>
<td class="mycell" style="text-align:right;">3,644,701</td>
</tr>

<tr>
<td class="mycell" align="left">Other non-articles ("File:", "Category:", "Wikipedia:", etc.)</td>
<td class="mycell" style="text-align:right;">2,479,330</td>
</tr>

<tr>
<td class="mycell" align="left"><b>Total</b></td>
<td class="mycell" style="text-align:right;">11,120,945</td>
</tr>

</table>

<p>So far, so good!</p>

<p>The remainder of this guide will provide a more principled approach
to working with Wikipedia.  However, for something "quick-and-dirty",
we can dump all articles into a flat text file with the following
command:</p>

<pre>
hadoop jar cloud9.jar edu.umd.cloud9.collection.wikipedia.DumpWikipediaToPlainText \
  -libjars bliki-core-3.0.16.jar,commons-cli-1.2.jar,commons-lang-2.6.jar,commons-validator-1.3.1.jar \
  -input Wikipedia/raw/enwiki-20110405-pages-articles.xml -output Wikipedia/txt
</pre>

<p>The output consist of text files, one article per line (article
title and contents, separated by a tab).</p>

</div></div>

<div class="post">
<h2>Sequentially-Numbered Docnos</h2>
<div class="content">

<p>Many information retrieval and other text processing tasks require
that all documents in the collection be sequentially numbered, from 1
... <i>n</i>.  Typically, you'll want to start with document 1 as
opposed to 0 because it is not possible to represent 0 with many
standard compression schemes used in information retrieval (i.e.,
gamma codes).</p>

<p>The next step is to create a mapping from Wikipedia internal
document ids (docids) to these sequentially-numbered ids (docnos).
Although, in general, docids can be arbitrary alphanumeric strings,
we'll be using Wikipedia internal ids (which happend to be integers,
but we'll treat as strings) as the docids.  This is a bit confusing,
so we'll illustrate.  The first full article in our XML dump file is:</p>

<pre>
  ...
  &lt;page&gt;
    &lt;title&gt;Anarchism&lt;/title&gt;
    &lt;id&gt;12&lt;/id&gt;
  ...
</pre>

<p>So this page gets docid "12" (the Wikipedia internal id) and docno
1 (the sequentially-numbered id).  We can build the mapping between
docids and docnos by the following command:</p>

<pre>
hadoop jar cloud9.jar edu.umd.cloud9.collection.wikipedia.BuildWikipediaDocnoMapping \
  -libjars bliki-core-3.0.16.jar,commons-cli-1.2.jar,commons-lang-2.6.jar,commons-validator-1.3.1.jar \
  -input Wikipedia/raw/enwiki-20110405-pages-articles.xml -output_path tmp \
  -output_file Wikipedia/docno-en-20110405.dat
</pre>

<p>The mapping file <code>docno-en-20110405.dat</code> is used by the
class <code>WikipediaDocnoMapping</code>, which provides a nice API
for mapping back and forth between docnos and docids. Note that in
this process, we're discarding all non-articles.</p>

<p>Aside: Why not use the article titles are docids?  We could, but
since we need to maintain the docid to docno mapping in memory (for
fast lookup), using article titles as docids would be expensive (lots
of strings to store in memory).</p>

</div></div>

<div class="post">
<h2>Repacking the Records</h2>
<div class="content">

<p>So far, we've been directly processing the raw uncompressed
Wikipedia data dump in XML.  To gain the benefits of compression
(within the Hadoop framework), we'll now repack the XML file into
block-compressed SequenceFiles, where the keys are the docnos, and the
values are <code>WikipediaPage</code> objects.  This will make subsequent
processing much easier.</p>

<p>Here's the command-line invocation:</p>

<pre>
hadoop jar cloud9.jar edu.umd.cloud9.collection.wikipedia.RepackWikipedia \
  -libjars bliki-core-3.0.16.jar,commons-cli-1.2.jar,commons-lang-2.6.jar,commons-validator-1.3.1.jar \
  -input Wikipedia/raw/enwiki-20110405-pages-articles.xml -output Wikipedia/compressed.block/en-20110405 \
  -mapping_file Wikipedia/docno-en-20110405.dat -compression_type block
</pre>

<p>In addition to block-compression, the program <code>RepackWikipedia</code> also
supports record-level compression as well as no compression:</p>

<pre>
# Record-level compression
hadoop jar cloud9.jar edu.umd.cloud9.collection.wikipedia.RepackWikipedia \
  -libjars bliki-core-3.0.16.jar,commons-cli-1.2.jar,commons-lang-2.6.jar,commons-validator-1.3.1.jar \
  -input Wikipedia/raw/enwiki-20110405-pages-articles.xml -output Wikipedia/compressed.record/en-20110405 \
  -mapping_file Wikipedia/docno-en-20110405.dat -compression_type record

# No compression
hadoop jar cloud9.jar edu.umd.cloud9.collection.wikipedia.RepackWikipedia \
  -libjars bliki-core-3.0.16.jar,commons-cli-1.2.jar,commons-lang-2.6.jar,commons-validator-1.3.1.jar \
  -input Wikipedia/raw/enwiki-20110405-pages-articles.xml -output Wikipedia/uncompressed/en-20110405 \
  -mapping_file Wikipedia/docno-en-20110405.dat -compression_type none
</pre>

<p>Here's how they stack up:</p>

<table>
<tr><td class="myheader"><b>Format</b></td>
    <td class="myheader"><b>Size (MB)</b></td>
</tr>

<tr>
<td class="mycell" align="left">Original XML dump (bzip2-compressed)</td>
<td class="mycell" style="text-align:right;">7,198</td>
</tr>

<tr>
<td class="mycell" align="left">Original XML dump (uncompressed)</td>
<td class="mycell" style="text-align:right;">31,719</td>
</tr>

<tr>
<td class="mycell" align="left">SequenceFiles (block-compressed)</td>
<td class="mycell" style="text-align:right;">6,636</td>
</tr>

<tr>
<td class="mycell" align="left">SequenceFiles (record-compressed)</td>
<td class="mycell" style="text-align:right;">8,358</td>
</tr>

<tr>
<td class="mycell" align="left">SequenceFiles (no compression)</td>
<td class="mycell" style="text-align:right;">20,693</td>
</tr>

</table>

<p>Note that during the repacking process we're throwing away
non-articles (so comparisons to the original XML dump are for
reference only). Block-compressed SequenceFiles are very space
efficient.  Record-compressed SequenceFiles are less space-efficient,
but retain the ability to directly seek to any
<code>WikipediaPage</code> object (whereas with block compression you
can only seek to block boundaries).  In most usage scenarios,
block-compressed SequenceFiles are preferred.</p>

</div></div>

<div class="post">
<h2>Supporting Random Access</h2>
<div class="content">

<p>Cloud<sup><small>9</small></sup> provides utilities for random
access to Wikipedia articles.  First, we need to build a forward
index:</p>

<pre>
hadoop jar cloud9.jar edu.umd.cloud9.collection.wikipedia.BuildWikipediaForwardIndex \
  -libjars bliki-core-3.0.16.jar,commons-cli-1.2.jar,commons-lang-2.6.jar,commons-validator-1.3.1.jar \
  -input /user/jimmylin/Wikipedia/compressed.block/en-20110405 -output tmp \
  -index_file Wikipedia/compressed.block/findex-en-20110405.dat
</pre>

<p><b>Note:</b> It mostly doesn't matter elsewhere, but
here <code>-input</code> must take an absolute path.</p>

<p>With this forward index, we can now programmatically access
Wikipedia articles given docno or docid.  Here's a snippet of code
that does this:</p>

<pre>
WikipediaForwardIndex f = new WikipediaForwardIndex();

f.loadIndex("/user/jimmylin/Wikipedia/compressed.block/findex-en-20110405.dat",
    "/user/jimmylin/Wikipedia/docno-en-20110405.dat");

WikipediaPage page;

// fetch docno
page = f.getDocument(1000);
System.out.println(page.getDocid() + ": " + page.getTitle());

// fetch docid
page = f.getDocument("1875");
System.out.println(page.getDocid() + ": " + page.getTitle());
</pre>

<p>This is illustrated by a simple command-line program that allows
you to find out the title of a page given either the docno or the
docid:</p>

<pre>
hadoop jar cloud9all.jar edu.umd.cloud9.collection.wikipedia.LookupWikipediaArticle \
  /user/jimmylin/Wikipedia/compressed.block/findex-en-20110405.dat \
  /user/jimmylin/Wikipedia/docno-en-20110405.dat
</pre>

<p>Note that the option <code>-libjars</code> doesn't work for this
program since it's not a MapReduce job.  As a result, we have to
package up all dependencies (i.e., Bliki) in the same jar.</p>

<p>Finally, there's also a web interface for browsing (the same
as <a href="clue-access.html#webapp">the random access webapp for
Clue</a>), which can be started with the following invocation:</p>

<pre>
hadoop jar cloud9.jar edu.umd.cloud9.collection.DocumentForwardIndexHttpServer \
  -libjars bliki-core-3.0.16.jar,commons-cli-1.2.jar,commons-lang-2.6.jar,commons-validator-1.3.1.jar \
  /user/jimmylin/Wikipedia/compressed.block/findex-en-20110405.dat /user/jimmylin/Wikipedia/docno-en-20110405.dat
</pre>

<p>When the server starts up, it'll report back the host information
of the node it's running on (along with the port). You can then direct
your browser at the relevant URL and gain access to the webapp.</p>

</div></div>

<!--- END MAIN CONTENT HERE -->

	</div>

	<!-- main END -->



		<div class="fixed"></div>

</div>

<!-- content END -->

<!-- footer START -->

<div id="footer">
<div id="copyright">
Last updated:
<script type="text/javascript">
<!--//
document.write(document.lastModified);
//-->
</script>
</div>

<div id="themeinfo">
Adapted from a WordPress Theme by <a href="http://www.neoease.com/">NeoEase</a>. Valid <a href="http://validator.w3.org/check?uri=referer">XHTML 1.1</a> and <a href="http://jigsaw.w3.org/css-validator/check/referer?profile=css3">CSS 3</a>.	</div>

</div>

<!-- footer END -->



</div>

<!-- container END -->

</div>

<!-- wrap END -->

</body>
</html>
